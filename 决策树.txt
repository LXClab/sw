在决策树中，对各节点提问题，并根据回答对节点进行分叉，从而实现分类数据的目的。
‘不纯度’（impurity）用作评估数据分离程度的标准，将一个节点数据划分为2个子节点时，最好的提问能够使子节点的不纯度降至最低。
一、分类与回归树
> library(rpart)
> m<-rpart(Species~.,data=iris)      #使用rpart()函数为鸢尾花数据创建决策树
> m
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)  
  2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
  3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)  
    6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
    7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
#结果中的n=150代表有150个数据。结果底部显示的就是决策树，缩进表示分支，*表示叶节点，loss表示误差数量。
从中可以得到结果：
鸢尾花数据中共有三个品种，各节点后括号里的数据代表了各个品种的比例
1节点为根节点
2节点的分类标准为Petal.Length<2.45,数量50，全是setosa品种
3节点的分类标准为Petal.Length>=2.45,数量100
6和 7节点都是由3节点划分的，标准为Petal.Width与1.75的关系

> plot(m,compress=T,margin=0.2)      #compress参数指定以更稠密的方式绘制决策树
> text(m,cex=1.5)
> library(rpart.plot)
> prp(m,type=4,extra=2,digits=3)#此处type参数为0~5

#从图中可以很清楚地看到分类和数量关系，其中品种下方的分数A/B表示B个数据中有A个属于该类（即上述loss误差）。
二、条件推断决策树
> library(party)
> m<-ctree(Species~.,data=iris)
> plot(m)


